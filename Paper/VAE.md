# VAE

我本来想做一期梳理VAE的视频，但是VAE这部分内容，一是数学公式太多，二是前后牵扯的相关知识太多太杂，所以还是写成文章先整理一下。

## From comprehension to generation

之前我们涉及的知识基本上都是理解类模型。什么叫理解类模型呢？比如给一个图片，我们想知道这个图片对应的类别是什么。我们还没涉及过生成类问题，比如说给我产生一个猫的图片。

这两种问题本质上其实是条件概率和联合概率的区别。理解类问题是研究$P(y|x)$的条件概率的问题，但是只研究条件概率的话，我们能解决的问题是有限的，只有掌握了$P(x,y)$这个联合概率分布我们才是掌握了全部的信息，理论上就能够解决所有问题。因为只要我们知道了$P(x,y)$的联合概率分布，$P(x)$、$P(y)$、$P(y|x)$、$P(x|y)$都可以通过联合概率分布得到。这样不管是“给我产生一个概率比较大的x”，还是“给我产生一个y的条件下概率比较大的x”，理论上都可以解决（实践上如何在可接受的时间内找到解是又一个研究的话题了）。

因此，生成问题就是在研究联合概率分布。总体来说有两类，一类是假定所有的随机变量都是可观察的，也就是说，我们只需要建模$P(x)$，其中x是数据集的样本点；另一类是我们假定我们能够观察到的随机变量并不是全部，还存在我们观察不到的隐变量，这时候我们要建模$P(x,y)$，其中x是数据集的样本点，y是隐变量。

前者的代表工作就是GPT，将$P(x)$分解为$P(x_0)P(x_1|x_0)P(x_2|x_0x_1)...P(x_n|x_0x_1..x_{n-1})$，然后用Transformer去拟合分解之后的条件概率；而后者的代表工作就是GAN和VAE，今天我们先讲VAE。

（感觉这里还是得重新梳理一下，到底我们要建模$P(x)$还是$P(x,y)$，什么时候有y什么时候没有y，为什么我感觉VAE里也没有y，好像到了CogView才有y的。）

我们先思考一个问题，既然我们都有神经网络了，万物皆可拟合，那我们何不直接去拟合$P(x)$？假定我们有一个神经网络$F(\cdot)$，输入x就可以输出概率，问题不就解决了？很好，怎么训练呢？最大似然估计。让给定的样本概率越大越好，让随机噪声样本概率越小越好。这样做有什么问题吗？啥问题也没有。只不过有时候我们需要的并不是给定一个x给出P(x)，我们想要的是按照x的概率分布去采样x。如果我们只有$F(\cdot)$，我们想采样只能遍历一遍所有可能，得到所有可能样本点的概率，然后按照这个概率分布去采样。这显然是不现实的。

这怎么办呢？走不通了，那我们就返回去思考：我们需要的到底是什么？在大多数实际情况下我们是需要一个能回答“概率比较大的x是什么”的函数。用白话来说，就是生成合理样本的能力。因此我们可以只建模数据的大致分布就可以，只需要对哪些样本点x是概率比较大、哪些样本点概率比较小有一个认知即可。

所以我们就可以用某些启发式搜索的方法尝试得到局部最大值，比如NLP里会用beam search，CV里可以对神经网络进行梯度上升（这个时候输入x成了训练的参数）。

并且这种条件的放松还有另一个好处，就是我们在训练的时候也不必对每一个可能的x进行建模概率，只需要建模数据集中的点和尽可能多的采样一些非样本点（而非遍历所有可能的非样本点）。我们记这种近似的函数为$\tilde F(\cdot)$，这个函数虽然不能精确把x的概率输出出来，但是样本之间的概率大小关系是差不多的准的，哪些点概率大一些、哪些点概率小一些，有个大致清楚的认知。

试想，我们去训练这样一个分类器：将数据集里的图片传进去，输出的概率越大越好，将随机噪声传进去，输出的概率越小越好。这样我们不就训练出来一个$\tilde F(\cdot)$了吗？这样我们就可以用启发式搜索来寻找概率较大的x了。

但是这种退而求其次的方法在实践中有时候产生的x会很奇怪。这是由于限于神经网络的拟合能力、以及数据的有限性，我们永远无法拟合出来完美的$\tilde F(\cdot)$，就算拟合出来了，我们也会发现，这个函数要找最大值（甚至局部最大值）都是一个很困难的问题（这个函数太复杂了）。

GAN就是为了增加数据的连续性而做了一个generator。而产生的generator恰好有生成图片的功能，就巧妙地生出了一个副产品。

另一种退而求其次的方法，我们去拟合一个一个的小的条件概率分布$P(x|z)$。我们的假设是，对于每个给定的x，都存在一个z，使得$x\sim\mathcal{N}(\text{Decoder}(z), \sigma^2 I)$。这种假设其实有一种Mixure of Gaussian的感觉，只要高斯分布足够多，任意的概率分布都可以拟合，而每个z就对应着一个拆解出来的高斯分布。

（未完待续）

## AE

其实对于普通的ML工程师来说，VAE就是AE（Auto-Encoder）给中间的编码加了一个正态分布随机噪声。这样理解也没有错，只不过对于做研究的人来说，只了解了表象是远远不够的，如果想对一个方法做出改进、提出一些创新，需要对事情的本质有更加深入的了解。

所以第一个问题，Auto-Encoder的本质是什么？

Auto-Encoder就是做了这样一个假设：对于一个数据点x，存在隐变量y，使得$x\sim \mathcal{N}(\text{Decoder}(y), \sigma^2I)$。这个假设是什么意思呢？意思就是我们假设x是服从某个正态分布的，这样我们就能去建模$P(x)$了，