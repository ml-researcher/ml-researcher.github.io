# VAE

我本来想做一期梳理VAE的视频，但是VAE这部分内容，一是数学公式太多，二是前后牵扯的相关知识太多太杂，所以还是写成文章先整理一下。

## From comprehension to generation

之前我们涉及的知识基本上都是理解类模型。什么叫理解类模型呢？比如给一个图片，我们想知道这个图片对应的类别是什么。我们还没涉及过生成类问题，比如说给我产生一个猫的图片。

生成类问题的本质是采样，这与理解类问题有本质的区别。理解类问题是给定x，我们想知道y，是一个条件概率问题$P(y|x)$。而生成问题是无中生有：给我产生一个合理的x。这个本质是对x的概率分布进行建模。

我们先思考一个问题，既然我们都有神经网络了，万物皆可拟合，那我们何不直接去拟合$P(x)$？假定我们有一个神经网络$F(\cdot)$，输入x就可以输出概率，问题不就解决了？很好，怎么训练呢？最大似然估计。让数据集中的样本概率越大越好，让非数据集样本概率越小越好。这样做有什么问题吗？啥问题也没有。只不过有时候我们需要的并不是给定一个x给出P(x)，我们想要的是按照x的概率分布去采样x。如果我们只有$F(\cdot)$，我们想采样只能遍历一遍所有可能，得到所有可能样本点的概率，然后按照这个概率分布去采样。这显然是不现实的。

这怎么办呢？走不通了，那我们就返回去思考：我们需要的到底是什么？在大多数实际情况下我们是需要一个能回答“概率比较大的x是什么”的函数。用白话来说，就是生成合理样本的能力。因此我们可以只建模数据的大致分布就可以，只需要对哪些样本点x是概率比较大、哪些样本点概率比较小有一个认知即可。

所以我们就可以用某些启发式搜索的方法尝试得到局部最大值，比如NLP里会用beam search，CV里可以对神经网络进行梯度上升（这个时候输入x成了训练的参数）。

并且这种条件的放松还有另一个好处，就是我们在训练的时候也不必对每一个可能的x进行建模概率，只需要建模数据集中的点和尽可能多的采样一些非样本点（而非遍历所有可能的非样本点）。我们记这种近似的函数为$\tilde F(\cdot)$，这个函数虽然不能精确把x的概率输出出来，但是样本之间的概率大小关系是差不多的准的，哪些点概率大一些、哪些点概率小一些，有个大致清楚的认知。

试想，我们去训练这样一个分类器：将数据集里的图片传进去，输出的概率越大越好，将随机噪声传进去，输出的概率越小越好。这样我们不就训练出来一个$\tilde F(\cdot)$了吗？这样我们就可以用启发式搜索来寻找概率较大的x了。

但是这种退而求其次的方法在实践中有时候产生的x会很奇怪。这是由于限于神经网络的拟合能力、以及数据的有限性，我们永远无法拟合出来完美的$\tilde F(\cdot)$，就算拟合出来了，我们也会发现，这个函数要找最大值（甚至局部最大值）都是一个很困难的问题（这个函数太复杂了）。

GAN就是为了增加数据的连续性而做了一个generator。而产生的generator恰好有生成图片的功能，就巧妙地生出了一个副产品。

另一种退而求其次的方法，我们去拟合一个一个的小的条件概率分布$P(x|z)$。我们的假设是，对于每个给定的x，都存在一个z，使得$x\sim\mathcal{N}(\text{Decoder}(z), \sigma^2 I)$。这种假设其实有一种Mixure of Gaussian的感觉，只要高斯分布足够多，任意的概率分布都可以拟合，而每个z就对应着一个拆解出来的高斯分布。

（未完待续）

## AE

其实对于普通的ML工程师来说，VAE就是AE（Auto-Encoder）给中间的编码加了一个正态分布随机噪声。这样理解也没有错，只不过对于做研究的人来说，只了解了表象是远远不够的，如果想对一个方法做出改进、提出一些创新，需要对事情的本质有更加深入的了解。

所以第一个问题，Auto-Encoder的本质是什么？

Auto-Encoder就是做了这样一个假设：对于一个数据点x，存在隐变量y，使得$x\sim \mathcal{N}(\text{Decoder}(y), \sigma^2I)$。这个假设是什么意思呢？意思就是我们假设x是服从某个正态分布的，这样我们就能去建模$P(x)$了，