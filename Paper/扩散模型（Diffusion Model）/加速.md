# 加速

这里主要关注非算法层面的加速，也就是说不考虑采样步数的优化等。

## 常用加速技巧

* https://huggingface.co/docs/diffusers/optimization/fp16
* https://www.photoroom.com/tech/stable-diffusion-25-percent-faster-and-save-seconds/

1. cuDNN auto-tuner
    1. 对于convolution操作，自动选一个当前最优的kernel（1%）
    2. 对于matmul操作，可以开启tf32
2. AMP：加一个autocast上下文，自动混合精度运算（72%）
3. 半精度权重（51%）
4. channel last memory format：不知道为什么可以加速（9.5%）
5. jit（2.7%）
    1. 可以用于模型的变换：https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html
    2. 编译优化
    3. c++提速
6. memory efficient attention：https://www.photoroom.com/tech/stable-diffusion-100-percent-faster-with-memory-efficient-attention/（22%？100%？）
7. TensorRT（25%）
    1. 转onnx，然后用onnx-simplifier简化，然后用TensorRT转格式
    



## 测时间的注意事项

1. 使用多次运行的min：https://stackoverflow.com/questions/24980500/which-metric-should-be-used-for-benchmarks/65430875#65430875
2. 调用time.time之前记得torch.cuda.synchronize：https://huggingface.co/docs/diffusers/optimization/fp16


## 其他方案

* https://github.com/facebookincubator/AITemplate/tree/main/examples/05_stable_diffusion（140%）
* https://github.com/VoltaML/voltaML-fast-stable-diffusion（150%）
* https://github.com/stochasticai/x-stable-diffusion

## AIT

你一旦踏入这个领域，你就会非常好奇，加速到底来自于哪里？

三方面的优化：https://horace.io/brrr_intro.html

* compute
* memory bandwidth
* overhead

所以AIT的优化来自于哪里呢？

