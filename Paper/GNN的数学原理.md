# GNN的数学原理

生在深度学习时代的researcher基本上都会用“搭积木式”的思维去理解各种方法，这个网络什么架构、那个网络什么架构、GCN的公式是这样、Transformer的公式是那样，etc。一个很重要的方面往往容易被忽视：这些架构背后的数学原理到底是什么？这些被我们忽略的数学部分，往往是科研想idea成功的关键。比如我们是否思考过为什么梯度下降会让参数收敛到局部最小值？为什么Transformer的自注意力会work？

这一期要分享的主题是GNN的数学原理。

（待续）