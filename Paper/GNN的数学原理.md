# GNN的数学原理

生在深度学习时代的研究者往往会用“搭积木式”的思维去理解各种方法，这个网络什么架构、那个网络什么架构、GCN的公式是这样、Transformer的公式是那样，etc。一个很重要的方面往往容易被忽视：这些架构背后的数学原理到底是什么？这些被我们忽略的数学部分，往往是科研想idea成功的关键。比如我们是否思考过为什么梯度下降会让参数收敛到局部最小值？为什么Transformer的自注意力会work？

这一期要分享的主题是GNN的数学原理。

但是让我感到痛苦的是，我在探究GNN的数学原理的过程中遇到了数学瓶颈。比如图的频域卷积这里严格的推导我实在是搞不清楚，光谱图论倒是学会了，可是傅里叶变换还是看不懂……目前正在重新学习数学分析，不知道会不会有用……如果有朋友偶然看到我这里的困惑，如果你有什么妙招，欢迎支招，万分感谢。

（待续）