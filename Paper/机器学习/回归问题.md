为什么回归问题使用最小均方误差损失函数？

因为最小均方误差等价于误差为正态分布时的最大似然估计。

我们假设$f(\cdot)$为神经网络学出来的某个函数，输入$x$可以得到预测值$\hat y=f(x)$。

我们期望$p(y=y^{(i)}|x^{(i)})$的概率越大越好（最大似然估计），因此需要表示出来这个概率。我们假设$p(y|x^{(i)};f(\cdot))\sim\mathcal{N}(f(x^{(i)}), \sigma^2)$，也就是

$$p(y|x^{(i)};f(\cdot)) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y-f(x^{(i)}))^2}{2\sigma^2}}$$

那么

$$p(y=y^{(i)}|x^{(i)};f(\cdot)) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-f(x^{(i)}))^2}{2\sigma^2}}$$

为了让这个概率最大，根据独立同分布假设，我们需要最大化

$$\prod p(y=y^{(i)}|x^{(i)};f(\cdot)) = \prod\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-f(x^{(i)}))^2}{2\sigma^2}}$$

取log，可以得到要最大化

$$
\begin{align*}
&&\sum\log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-f(x^{(i)}))^2}{2\sigma^2}} \\
=&& \sum \log \frac{1}{\sqrt{2\pi}\sigma} + \sum -\frac{(y^{(i)}-f(x^{(i)}))^2}{2\sigma^2} \\
=&& \sum \log \frac{1}{\sqrt{2\pi}\sigma} + \frac{1}{\sigma^2}\sum -\frac{(y^{(i)}-f(x^{(i)}))^2}{2}
\end{align*}
$$

由此可见，最大化似然函数等价于最小化均方误差。