# 线性回归

## 损失函数

线性回归的损失函数是最小均方误差：

$$J(\theta) = \frac{1}{2}\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2$$

为什么回归问题使用最小均方误差损失函数？

因为最小均方误差等价于误差为正态分布时的最大似然估计。

我们假设$f(\cdot)$为神经网络学出来的某个函数，输入$x$可以得到预测值$\hat y=f(x)$。

我们期望$p(y=y^{(i)}|x^{(i)})$的概率越大越好（最大似然估计），因此需要表示出来这个概率。我们假设$p(y|x^{(i)};f(\cdot))\sim\mathcal{N}(f(x^{(i)}), \sigma^2)$，也就是

$$p(y|x^{(i)};f(\cdot)) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y-f(x^{(i)}))^2}{2\sigma^2}}$$

那么

$$p(y=y^{(i)}|x^{(i)};f(\cdot)) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-f(x^{(i)}))^2}{2\sigma^2}}$$

为了让这个概率最大，根据独立同分布假设，我们需要最大化

$$\prod p(y=y^{(i)}|x^{(i)};f(\cdot)) = \prod\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-f(x^{(i)}))^2}{2\sigma^2}}$$

取log，可以得到要最大化

$$
\begin{align*}
&&\sum\log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-f(x^{(i)}))^2}{2\sigma^2}} \\
=&& \sum \log \frac{1}{\sqrt{2\pi}\sigma} + \sum -\frac{(y^{(i)}-f(x^{(i)}))^2}{2\sigma^2} \\
=&& \sum \log \frac{1}{\sqrt{2\pi}\sigma} + \frac{1}{\sigma^2}\sum -\frac{(y^{(i)}-f(x^{(i)}))^2}{2}
\end{align*}
$$

由此可见，最大化似然函数等价于最小化均方误差。

## 梯度下降法求解线性回归

$$
\begin{align*}
\frac{\partial J}{\partial \theta_j} =&& \frac{\partial}{\partial \theta_j} [\frac{1}{2}\sum (f(x^{(i)})-y^{(i)})^2] \\
=&& \sum \frac{1}{2}\cdot 2 (f(x^{(i)})-y^{(i)}) \frac{\partial }{\partial \theta_j}[f(x^{(i)})-y^{(i)}] \\
=&& \sum (f(x^{(i)})-y^{(i)}) \frac{\partial}{\partial \theta_j}[x^{{(i)}T} \theta - y^{(i)}] \\
=&& \sum (f(x^{(i)})-y^{(i)}) x^{(i)}_j
\end{align*}
$$

写成矩阵的形式：

$$
\frac{\partial J}{\partial \theta} = X^T (X\theta-Y)
$$

$$
\theta = \theta - \alpha \frac{\partial J}{\partial \theta}
$$

## 表达式直接求解线性回归

令导数为0，可得

$$
X^TX\theta = X^TY
$$

即

$$
\theta = (X^TX)^{-1}X^T Y
$$

## 参考资料

* https://cs229.stanford.edu/lectures-spring2022/main_notes.pdf