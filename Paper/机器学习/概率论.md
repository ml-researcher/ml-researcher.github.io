概率论诡辩

我发现自己开始形成批判性思维了（长脑子了？）。

训练神经网络我们通常使用最大似然估计，但是实际上还有其他参数估计方法。

使用最大似然估计的时候我们通常基于独立同分布假设，但是实际上独立是不存在的。比如很简单的抛硬币实验，如何保证每次抛硬币都是独立的？谁能保证这个硬币每被抛一次，概率不会基于前一次的结果发生变化？所以我们理论中学习的实际叫做“理想实验”。

我们作为应用的开发者，只能做到能用。如果研究理论，需要形式化地表述一切。

## 参数估计方法

https://zhuanlan.zhihu.com/p/523669354

我们需要确认一下符号表示。$p(x)$代表x的概率，$p(\cdot)$是一个确定的概率分布。而$p(x;\theta)$表示在参数$\theta$下的x的概率，也是一个概率分布。

我们有数据集$\mathcal{X}=\{x_1, x_2,\dots,x_n\}$。

### 最大似然估计

选择一个$p(\cdot;\theta)$估计$p(\cdot)$。选择的标准是

$$
\theta^* = \underset{\theta\in \Theta}{\mathrm{argmax~}} p(\mathcal{X};\theta)
$$

为什么这样选择呢？其实没有什么道理。一个常见的解释是“让发生的（观测到的）结果概率最大”，可是这种解释仍然是经验主义的。真正的答案是：没有人能保证这样选择出来的就是最好的估计，因为观测值本身是随机变量的采样，具有随机性。之所以人们用，是因为实践证明好用。

### 最大后验估计

这里会牵扯到什么是随机变量，随机变量的本质到底是什么。

随机变量是不确定的变量，它的不确定性可能来自于：

* 还未发生导致的不确定性：比如明天的天气
* 发生了但无法观测导致的不确定性：比如$\theta^*$

要特别区分**随机变量**和**随机变量的值**。发生了的可以观测的就是确定的了，就不再是随机变量，比如$\mathcal{X}$。

只有随机变量的值才能放在概率函数的括号里，这里规范一下符号表示$p_x(\cdot)$表示随机变量x的概率分布，$p_x(x_0)$表示x取$x_0$的概率值，在没有歧义的情况下也可以写作$p(x_0)$。比如$\mathcal{X}$的概率可以写作$p(\mathcal{X})$。

我们可以说$\theta^*$是随机变量，因为无法观测。但我们不能说$\theta$是随机变量，因为$\theta$就是确定的参数。（这也是很多解释最大后验估计的时候的误区！）如果我们把$\theta$看作随机变量$\theta^*$的可能取值，那么$\theta$就成了随机变量的值，也就可以放在概率函数的括号里了。

解释清楚了概念，就可以来形式化地定义最大后验估计到底在做什么了。

选择一个$p(\cdot|\theta)$估计$p(\cdot)$。选择的标准是

$$
\theta^* = \underset{\theta\in \Theta}{\mathrm{argmax~}} p(\theta|\mathcal{X})
$$

这里有一点区别$p(\cdot|\theta)$和$p(\cdot;\theta)$，我觉得这两个本质是一样的，可以认为两者没有区别，只是习惯将参数放在分号后面表示。

根据贝叶斯公式：

$$
p(\theta|\mathcal{X}) = \frac{p(\mathcal{X}|\theta)p(\theta)}{p(\mathcal{X})}
$$

分母是一个常数，不需要优化，分子的$p(\theta)$通常被建模为一个已知的先验，也不需要优化。因此选择的标准和最大似然很像，只是乘以了一个权重$p(\theta)$。换句话说，最大似然就是在假设$p(\theta)$服从均匀分布的情况下进行的最大后验估计。

## 贝叶斯估计

贝叶斯估计在最大后验估计的基础上去掉了$\arg\max$操作，直接得到$\theta^*$的概率分布。