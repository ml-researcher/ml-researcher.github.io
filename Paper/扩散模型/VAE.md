# VAE

这应该是第三次总结VAE。第一次是在[b站](https://www.bilibili.com/video/BV1Z64y167iJ)用视频总结的，第二次是在[github](https://github.com/ml-researcher/VAE)整理了一份pdf。感觉自己断断续续学了很久VAE，可是每次学都会有新的疑问，看来VAE真可谓是常看常新了。

## 先验

在处理概率问题的时候我们需要去分清哪些是客观存在的（虽然本质上来说一切都是客观存在的），或者换一种说法，哪些是我们需要去构造的，哪些是我们需要去估计的。

$$
\begin{flalign*}
\log p(x) &= \log \int_\mathcal{Z} p(x|z)p(z) dz \\
&= \log \int_\mathcal{Z} q(z) \frac{p(x|z)p(z)}{q(z)} dz \\
&= \log E_{z\sim q(z)} [\frac{p(x|z)p(z)}{q(z)}] \\
&\ge E_{z\sim q(z)} [\log \frac{p(x|z)p(z)}{q(z)}] && \text{(Jensen inequity)} \\
&= E_{z\sim q(z)}[\log p(x|z)] - KL(q(z)\| p(z))
\end{flalign*}
$$

概率公式其实表达了数据是如何生成的，比如上面第一步，就表明了数据x是由隐变量z生成的，给定z以后，x就有一个对应的条件概率分布。

这里的$p(z)$和$p(x|z)$都是客观存在的，只是我们不知道它们到底是什么（除了上帝没人知道）。我们能做的只有两件事：猜想+统计估计。比如在VAE中，$p(z)$就是猜想的先验，一般来说是高斯分布，或者VQ-VAE里是离散词表均匀分布。对于$p(x|z)$，一般来说是用神经网络拟合的正态分布。

ELBO里最重要的一点是无中生有了一个$q(z)$出来，或者完整写出来是$q(z|x)$。

首先要接受这样一个事实：在所有可能的概率分布中，一定存在某个分布$q(z)$使得ELBO最大，只是我们不知道这个分布是什么。因此我们用神经网络去预测这个分布。

这里会涉及两个很本质的问题：我们期望神经网络预测的是什么？为什么优化ELBO就可以达到这个期望？

第一个问题：我们有两个神经网络Enc和Dec，Enc用来预测给定x以后最优的$q(z)$，Dec用来预测给定z以后x的条件概率分布。

第二个问题：这里用到一个很关键的思想——迭代。我们假设Enc已经实现了它的目标，那么优化ELBO就可以让Dec接近期望；我们假设Dec已经实现了它的目标，那么优化ELBO就可以让Enc接近期望。

## 分布的变换

这里又是一个很本质的问题：为什么ELBO的概率公式对应着数据分布的变换。

这里就必须引入ELBO的另一个推导过程：直接用等式推导，而不是用Jensen不等式推导。

$$
\begin{align*}
\log p(x) &= \int_{\mathcal{Z}} q(z)\log \frac{p(x,z)q(z)}{p(z|x)q(z)} dz \\
&= E_{z\sim q(z)}[\log \frac{q(z)}{p(z|x)}] - E_{z\sim q(z)}[\log \frac{q(z)}{p(x,z)}] \\
&= KL(q(z)\| p(z|x)) - KL(q(z)\| p(x,z))
\end{align*}
$$

等式的第一项是gap，第二项就是ELBO，于是有：

$$
ELBO = \log p(x) - KL(q(z)\| p(z|x))
$$

根据这个等式，让ELBO最大的最优的$q(z)$恰好就是$p(z|x)$。这也就意味着我们期望神经网络Enc学完以后，就是可以给定x预测z的条件概率分布（后验预测）。

-------

第四次总结。

感觉每次都不得要领，没有理解问题的本质。这次一定尽力做到。

这次的参考文献是Understanding Diffusion Models: A Unified Perspective. Calvin Luo

问题1：概率图的意义是什么。

x是观测值，z是隐变量，它们是**相关的**。从本质上讲，它们没有区别，都是随机变量。唯一的区别就是x可以被观测到，z不能被观测到。

因为x和z是相关的，所以它们之间有条件概率分布，也就是给定其中一个，另一个的概率分布会发生偏移。这里会涉及到概率的含义（知识的引入会让概率发生偏移，先验和后验）。

会涉及到的分布：p(x), p(z), p(x|z), p(z|x)。习惯上，我们用p表示真实的概率分布，q表示我们用于近似真实的分布，比如$q_\phi(z|x)$。

接下来讨论“我们有什么”以及“我们要得到什么”。

从现实的角度，我们只有有限的样本x。（有些时候我们有x,z的对应）

先讨论只有样本x的情况，我们要得到的就是一个generator，这个generator可以从p(x)里采样x，从而生成样本。VAE的假设是，生成x的过程可以通过z来间接完成。先采样一个z，根据p(x|z)来产生x。问题来了：为什么不直接生成x，还要通过z来间接产生x呢？答案是当然可以，你可以以任意的方式去产生x，因为除了上帝没有人知道x究竟是怎么产生的。比如，直接对x本身进行建模，基于x是正态分布的先验去拟合均值、标准差，这就是高斯模型。高斯模型可以直接通过最大似然估计得到解析解。直接对p(x)建模的缺点是我们需要给x加上强prior。为了让x的prior弱一些，我们转而建模p(x|z)。这样建模的好处是：把x的生成过程变成了一个输入z输出x的函数运算过程，而函数运算就可以通过神经网络建模了。

概率图的本质就是一堆随机变量之间的关系，这些随机变量没有本质区别，只是相关的变量之间，如果已知某一个会对另一个的概率产生偏移。

什么是层次VAE？回忆上面讲的“为了让x的prior弱一些，我们转而建模p(x|z)”。进一步，为了让z的prior弱一些，我们转而建模p(z|z_1)……理论上层次VAE是一个拟合能力无限大的建模方式，所以有时候我们需要限制他的能力，比如扩散模型。扩散模型就是加入了以下限制的层次VAE：

* x和z的维度相同
* q(z|x)以及q(z_i|z_i-1)都是确定的，不需要学习
* p(z_T)~N(0,1)

我们如果对p(z|x)没有任何知识，理论上讲优化ELBO和优化似然是一样的。（那如果对p(z|x)有一些知识呢？）

这里有一个很关键的概念：优化ELBO等价于优化似然！我之前一直觉得优化ELBO是近似优化似然，实际上不是的。$\log p(x)$是一个客观常量，$ELBO+KL$也就是一个常量，ELBO变大KL必定会变小，让ELBO最大的结果必然是和似然最大的结果一致，并且我们通过优化还得到了一个KL很小的后验拟合器。

这里必须吐槽一下论文里的数学表达，确实不够严谨。我们用$\log p(x)$表示真实的概率，用$\log p_\theta (x)$表示用神经网络$\theta$拟合的概率，这两个值并不是相等的关系，我们期望做到的是$\log p(x)=\log p_{\theta^*}(x)$。至于能否真的做到，我们也不得而知，因为$\log p(x)$只有上帝知道。

最大似然估计的想法是让观测到的发生的概率最大。并不能说这种方式就是最好的，还有其他的估计方法比如贝叶斯估计。（感觉越扯越远了，看来需要重新整理了……）

另一个容易混淆的点就是函数映射、概率分布、采样之间的关系。