# 加速

这里主要关注非算法层面的加速，也就是说不考虑采样步数的优化等。

## 常用加速技巧

* https://huggingface.co/docs/diffusers/optimization/fp16
* https://www.photoroom.com/tech/stable-diffusion-25-percent-faster-and-save-seconds/

1. cuDNN auto-tuner
    1. 对于convolution操作，自动选一个当前最优的kernel（1%）
    2. 对于matmul操作，可以开启tf32
2. AMP：加一个autocast上下文，自动混合精度运算（72%）
3. 半精度权重（51%）
4. channel last memory format：不知道为什么可以加速（9.5%）
5. jit（2.7%）
    1. 可以用于模型的变换：https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html
    2. 编译优化
    3. c++提速
6. memory efficient attention：https://www.photoroom.com/tech/stable-diffusion-100-percent-faster-with-memory-efficient-attention/（22%？100%？）
7. TensorRT（25%）
    1. 转onnx，然后用onnx-simplifier简化，然后用TensorRT转格式


## 其他方案

* https://github.com/facebookincubator/AITemplate/tree/main/examples/05_stable_diffusion（140%）
* https://github.com/VoltaML/voltaML-fast-stable-diffusion（150%）
* https://github.com/stochasticai/x-stable-diffusion
* https://mp.weixin.qq.com/s/KqffXfRhLN0LP3cHKkt2DQ
* https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/

这里主要关注inference的加速（也就是不需要反向传播）。（至于是否考虑输入相关的控制流图再说，先暂时考虑静态图。）

我觉得最痛苦的就是这是一个很新的领域，文档和教程太少！慢慢探索吧，加油。还有一个问题就是会踩坑……


你一旦踏入这个领域，你就会非常好奇，加速到底来自于哪里？

三方面的优化：https://horace.io/brrr_intro.html

* compute
* memory bandwidth
* overhead

（为了更加细化上面的理解，我需要更加详细地知道一些概念比如cuDNN、tensor core、cublas、cuda runtime、onnx啥的）

我觉得对于每一个库，我都需要知道：

* 这个库的功能是什么？可以输入什么输出什么？
* 这个库是如何实现这个功能的？原理是什么？
* 如何使用这个库？

## TensorRT

tensorrt / nvidia-tensorrt / torch_tensorrt区别是啥？

* 首先，计算图的相同操作合并（水平融合）、冗余节点的删除很容易理解，计算量本身变少了
* 有一点我没有理解，不考虑反向传播的话，算子融合在哪里起作用？难道是控制RAM搬运？
    * 我知道了，是常量的提前计算，就好比两个linear合成一个linear，因为是inference！
    * 当然，in-place操作也可以减少RAM搬运
* 固定batch size可以有额外的优化，小bs低延迟、大bs高吞吐量
* 半精度，inference对精度要求很低因为不用反向传播。

onnx的流程和图有什么关系呢？trt在哪个步骤起作用？

pytorch模型->onnx->onnx-simplifier
    ->直接用pycuda手动分配显存让engine跑起来
    ->或者用torchtrtc转成ts（torch_tensorrt配环境太坑了……血泪教训，学docker！）

配置环境的时候越来越觉得自己对gpu和cuda的了解太浅了：

* 为什么torch.version.cuda=11.5可以运行在cuda driver只有11.1的机器上？

## AIT

https://ai.facebook.com/blog/gpu-inference-engine-nvidia-amd-open-source/

## TorchScript

https://www.youtube.com/watch?v=2awmrMRf0dA

主要解决portable和performance问题。

F.dropout(inputs, training=self.training)

ts会从code层面进行优化，思路是将model转成script，然后优化script。

* Algebraic rewriting: constant folding, common sub-expression elimination, dead code elimination
* Reorder operation to reduce memory pressure and make use of cache locality
* Fusion: fuse operations into a single kernel to reduce overhead
* 硬件相关的code generation

不仅可以优化inference，还可以优化training。

优化了以后甚至比手写cuDNN还快。

## Triton OpenAI

