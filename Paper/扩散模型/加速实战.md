# 加速实战

## 基建

* 比较方法的输入输出是否一致
* 比较方法的运行速度（包括选择运行次数、比较方法取最小值还是平均值）
* 选择输入的分布
* 注意事项的实现
    1. 使用多次运行的min：https://stackoverflow.com/questions/24980500/which-metric-should-be-used-for-benchmarks/65430875#65430875
    2. 调用time.time之前记得torch.cuda.synchronize：https://huggingface.co/docs/diffusers/optimization/fp16
    3. 调用之前记得warmup：https://pytorch.org/TensorRT/_notebooks/CitriNet-example.html
    4. forward之前记得开eval()以及记得with torch.no_grad。
* 实际情况下，肯定是先把原模型改写成trt兼容的写法，所以需要验证改写的正确性

## ldm

model是LatentDiffusion

model.model是DiffusionWrapper，用的是c_crossattn，暂时还没理解为什么DiffusionWrapper是一个LightningModule

model.model.diffusion_model是UNet，也是我需要去加速的核心位置。

* 输入有三个参数：x，t和context
* x: (bs*2, 4, 64, 64) float32
* t: (6,) int64 tensor([981, 981, 981, 981, 981, 981], device='cuda:0')
* context: (bs*2, 77, 768) float32

## TensorRT

我需要尝试各种方法拆解小模块，看一下小模块的效果，比如说timestamp

尝试几种方式的对比：（多个dimension的排列组合）

* jit.trace
* jit.script
* nn.module
* function

导出

* 导出onnx，再转成trt，再转回ts
* 直接用ts转成trt的ts，compile

各种坑。。。减少implicit的操作，比如类型转换，尽量用trace