# 加速实战

## 基建

* 比较方法的输入输出是否一致
* 比较方法的运行速度（包括选择运行次数、比较方法取最小值还是平均值）
* 选择输入的分布
* 注意事项的实现
    1. 使用多次运行的min：https://stackoverflow.com/questions/24980500/which-metric-should-be-used-for-benchmarks/65430875#65430875
    2. 调用time.time之前记得torch.cuda.synchronize：https://huggingface.co/docs/diffusers/optimization/fp16
    3. 调用之前记得warmup：https://pytorch.org/TensorRT/_notebooks/CitriNet-example.html
    4. forward之前记得开eval()以及记得with torch.no_grad。
* 实际情况下，肯定是先把原模型改写成trt兼容的写法，所以需要验证改写的正确性

## ldm

model是LatentDiffusion

model.model是DiffusionWrapper，用的是c_crossattn，暂时还没理解为什么DiffusionWrapper是一个LightningModule

model.model.diffusion_model是UNet，也是我需要去加速的核心位置。

* 输入有三个参数：x，t和context
* x: (bs*2, 4, 64, 64) float32
* t: (6,) int64 tensor([981, 981, 981, 981, 981, 981], device='cuda:0')
* context: (bs*2, 77, 768) float32

## TensorRT

我需要尝试各种方法拆解小模块，看一下小模块的效果，比如说timestamp

尝试几种方式的对比：（多个dimension的排列组合）

* jit.trace
* jit.script
* nn.module
* function

导出

* 导出onnx，再转成trt，再转回ts
* 直接用ts转成trt的ts，compile（不太好用，一些算子不支持，比如to(device)）

转torchscript确实是一个技术活，有太多需要注意的事情了。

通用注意事项：

* 尽量不要有implicit操作，比如type cast要写明确。（torch.log(int64)）

jit.trace注意事项：

* 只支持tensor作为参数

jit.script注意事项：

* 尽量用原生pytorch，不要用其他包比如transformers、einops。
* 即使是if分支，输出的shape和type都要保持一致，最后一维如果有if可以最后一维大小不一样。（batch维度怎么动态还得再研究）

我需要知道：

* jit.script是否会执行代码？如果不执行的话一些变量的值怎么知道呢？（不是input的值）可以认为jit.script会执行，非输入的值会被认为是常量。

torch_tensorrt要求所有输入都是tensor，tensorrt有这个要求吗？

经过测试timestep函数经过trt优化以后有8倍的时间增长。。需要探究一下是什么原因：

* 可能是fp16和fp32之间的类型转换问题
* 可能是trt转ts的时候产生的overhead（应该是这个，直接用trtexec测试就快）